\documentclass[memoire.tex]{subfiles}

\chapter*{Conclusion}
Le domaine de l'apprentissage est un champs d'étude particulièrement utilisé. De la reconnaissance d'image à la recommandation, il a fait l'objet de nombreuses recherches. Dans ce mémoire, nous avons abordé la partie supervisée de cet apprentissage et plus particulièrement, la notion de classement. Celle-ci est basée sur des concepts de probabilité, tels que le théorème de Bayes ou le théorème des probabilités totales. Par ailleurs, le cas le plus représenté est celui de l'arbre de décision. Nous avons vu qu'il était possible de quantifier l'information afin de déterminer une classe à une instance. Cependant le problème d'\textit{overfitting} rend cette décision de moins en moins fiable. C'est pourquoi des solutions d'élagage sont alors apparus afin de réduire l'impact de l'\textit{overfitting}.\\

Dans un deuxième temps, nous avons vu trois algorithmes de classement : \textit{K-Nearest Neighbors}, \textit{Classification And Regression Trees} et C4.5. Chacun possède ses avantages et ses inconvénients et suite à une mise en pratique, il a été possible de les mettre en valeur. Grâce au MNIST, il a été possible de montrer que K-NN est précis mais couteux tandis que CART est rapide mais pas assez fiable. Dans notre problématique de reconnaissance de numéros étudiant, il a été décidé de retenir K-NN. Celui-ci semble plus adéquat étant donné la qualité du classement et l'absence de nécessité à avoir une solution particulièrement rapide.\\

Il existe cependant de nombreuses autres techniques d'amélioration des algorithmes (\textit{bagging}, \textit{boosting}) ainsi que d'autres méthodes (\textit{Lazy Decision Tree}, \textit{Random Forest}, algorithmes génétiques, etc.) mais celles-ci font l'objet de recherches complémentaires. Il n'existe pas de solution universelle dans l'apprentissage supervisée. La réalité fait qu'il est souvent très compliqué d'appliquer toutes ces notions à des cas existants. Dans le nôtre, les recherches se sont basées sur le fait d'avoir toutes les données. K-NN pourrait ne pas être la meilleure solution si des données étaient manquantes ou si les données changeaient.